# ğŸ¯ 95%+ å‡†ç¡®ç‡é…ç½®è¯´æ˜

## æ¨¡å‹æ¶æ„ä¼˜åŒ– (å·²å†…ç½®)

```python
# æ–°çš„é»˜è®¤å‚æ•° (models/bert_model.py)
vocab_size = 6000              # æ›´å¤§è¯æ±‡è¡¨è¦†ç›–
d_model = 768                  # BERT-baseçº§åˆ« (vs æ—§512)
num_layers = 8                 # æ›´æ·± (vs æ—§6å±‚)
num_heads = 12                 # æ›´å¤šæ³¨æ„åŠ› (vs æ—§8å¤´)
d_ff = 3072                    # 4å€æ¨¡å‹ç»´åº¦ (vs æ—§2048)
max_length = 1024              # è¦†ç›–95%æ ·æœ¬ (vs æ—§512)
dropout = 0.15                 # ä¼˜åŒ–è¿‡æ‹Ÿåˆ (vs æ—§0.1)
batch_size = 16                # é…åˆæ¢¯åº¦ç´¯ç§¯
learning_rate = 2e-5           # æ›´ç¨³å®š (vs æ—§1e-4)
epochs = 15                    # å……åˆ†è®­ç»ƒ (vs æ—§10)

# æ–°å¢é«˜çº§æŠ€æœ¯
warmup_ratio = 0.1             # 10%æ­¥æ•°warmup
label_smoothing = 0.1          # æ ‡ç­¾å¹³æ»‘
gradient_accumulation = 2      # æœ‰æ•ˆbatch=32
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹æ¡ˆ1: ç›´æ¥è¿è¡Œè„šæœ¬ (æ¨è)

```bash
chmod +x train_high_accuracy.sh
./train_high_accuracy.sh
```

### æ–¹æ¡ˆ2: æ‰‹åŠ¨è¿è¡Œå•ä¸ªé…ç½®

#### æ¨èé…ç½® (å¹³è¡¡ç‰ˆ)

```bash
python main.py train \
    --model-spec bert \
    --nrows 100000 \
    --epochs 15 \
    --batch-size 16 \
    --learning-rate 2e-5 \
    --model-out models/model_bert_high_balanced.pt
```

**é¢„æœŸ**: 3-4å°æ—¶ï¼Œå‡†ç¡®ç‡ 92-95%

#### ç»ˆæé…ç½® (å®Œæ•´ç‰ˆ)

```bash
python main.py train \
    --model-spec bert \
    --epochs 15 \
    --batch-size 16 \
    --learning-rate 2e-5 \
    --model-out models/model_bert_ultimate.pt
```

**é¢„æœŸ**: 8-12å°æ—¶ï¼Œå‡†ç¡®ç‡ 95-97%

## ğŸ”§ å…³é”®ä¼˜åŒ–ç‚¹

### 1. æ¨¡å‹å®¹é‡ç¿»å€

- å‚æ•°é‡: 21M â†’ 40M
- ç»´åº¦: 512 â†’ 768 (BERT-baseæ ‡å‡†)
- æ·±åº¦: 6å±‚ â†’ 8å±‚

### 2. å­¦ä¹ ç‡è°ƒåº¦

```python
# Warmup (å‰10%æ­¥æ•°çº¿æ€§å¢é•¿)
if step < warmup_steps:
    lr = base_lr * (step / warmup_steps)

# Cosine Decay (åç»­ä½™å¼¦è¡°å‡)
else:
    progress = (step - warmup_steps) / (total_steps - warmup_steps)
    lr = base_lr * 0.5 * (1 + cos(Ï€ * progress))
```

### 3. æ ‡ç­¾å¹³æ»‘

```python
# åŸå§‹: [0, 0, 1, 0, 0]
# å¹³æ»‘: [0.007, 0.007, 0.964, 0.007, 0.007]
# æ•ˆæœ: å‡å°‘è¿‡æ‹Ÿåˆï¼Œæå‡æ³›åŒ–
```

### 4. æ¢¯åº¦ç´¯ç§¯

```python
# å®é™…batch=16ï¼Œç´¯ç§¯2æ­¥
# ç­‰æ•ˆbatch=32ï¼Œå‡å°‘å†…å­˜å ç”¨
for step in range(0, len(data), 16):
    loss = model(batch) / 2
    loss.backward()
    if (step + 1) % 2 == 0:
        optimizer.step()
        optimizer.zero_grad()
```

### 5. æ›´é•¿åºåˆ—

- æ—§: 512 tokens (è¦†ç›–90%æ ·æœ¬)
- æ–°: 1024 tokens (è¦†ç›–95%æ ·æœ¬)
- æ•ˆæœ: æ›´å®Œæ•´çš„ä¸Šä¸‹æ–‡ç†è§£

## ğŸ’¡ è¿›ä¸€æ­¥æå‡å»ºè®®

å¦‚æœ95%è¿˜ä¸å¤Ÿï¼Œå¯ä»¥å°è¯•ï¼š

### 1. é›†æˆå­¦ä¹ 

```bash
# è®­ç»ƒ3ä¸ªæ¨¡å‹
python main.py train --model-spec bert --model-out models/bert_1.pt --epochs 15
python main.py train --model-spec bert --model-out models/bert_2.pt --epochs 15
python main.py train --model-spec bert --model-out models/bert_3.pt --epochs 15

# é›†æˆé¢„æµ‹ (éœ€è¦è‡ªå·±å®ç°)
# æŠ•ç¥¨æˆ–å¹³å‡æ¦‚ç‡
```

### 2. æ•°æ®å¢å¼º

- éšæœºåˆ é™¤token (10%)
- éšæœºäº¤æ¢ç›¸é‚»token
- å›è¯‘ (å¦‚æœæœ‰æ˜ å°„è¡¨)

### 3. ç±»åˆ«æƒé‡

```python
# é’ˆå¯¹ç±»åˆ«ä¸å¹³è¡¡
class_weights = compute_class_weight('balanced', classes=unique_labels, y=labels)
criterion = nn.CrossEntropyLoss(weight=class_weights)
```

### 4. Focal Loss

```python
# èšç„¦éš¾åˆ†ç±»æ ·æœ¬
class FocalLoss(nn.Module):
    def __init__(self, gamma=2.0):
        super().__init__()
        self.gamma = gamma

    def forward(self, input, target):
        ce_loss = F.cross_entropy(input, target, reduction='none')
        pt = torch.exp(-ce_loss)
        focal_loss = ((1 - pt) ** self.gamma * ce_loss).mean()
        return focal_loss
```

## ğŸ“ è®­ç»ƒç›‘æ§

è®­ç»ƒæ—¶å…³æ³¨ï¼š

- âœ… **è®­ç»ƒå‡†ç¡®ç‡**: åº”æŒç»­ä¸Šå‡åˆ°95%+
- âœ… **éªŒè¯å‡†ç¡®ç‡**: ç›®æ ‡95%+
- âœ… **Lossæ”¶æ•›**: åº”æŒç»­ä¸‹é™
- âœ… **å­¦ä¹ ç‡æ›²çº¿**: Warmupåå¹³æ»‘ä¸‹é™
- âš ï¸ **è¿‡æ‹Ÿåˆä¿¡å·**: Train accè¿œé«˜äºVal acc

## ğŸ“ æ¨ç†ä½¿ç”¨

```bash
# ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹
python main.py infer \
    --model models/model_bert_ultimate.pt \
    --model-type bert \
    --input-csv data/test_a.csv \
    --output-csv predictions.csv
```
