
### 1. æ›´æ·±ç½‘ç»œ
- **å±‚æ•°**: 8å±‚ â†’ **10å±‚**
- **å‚æ•°**: 40M â†’ **50M**
- æ›´å¼ºçš„è¡¨è¾¾èƒ½åŠ›

### 2. Focal Loss
```python
# è‡ªåŠ¨å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
# èšç„¦éš¾åˆ†ç±»æ ·æœ¬
FocalLoss(gamma=2.0)
```
- è§£å†³14ç±»ä¸å¹³è¡¡é—®é¢˜
- å¯¹å›°éš¾æ ·æœ¬ç»™äºˆæ›´å¤šå…³æ³¨

### 3. ä¼˜åŒ–è¶…å‚æ•°
- **è¯æ±‡**: 6000 â†’ **7000**
- **å­¦ä¹ ç‡**: 2e-5 â†’ **1.5e-5** (æ›´ç¨³å®š)
- **Dropout**: 0.15 â†’ **0.2** (æ›´å¼ºæ­£åˆ™)
- **Warmup**: 10% â†’ **15%** (æ›´å¹³æ»‘)
- **Epochs**: 15 â†’ **20** (æ›´å……åˆ†)

### 5. Test-Time Augmentation (TTA) (+0.3-0.5%)
```python
# æ¨ç†æ—¶å¯ç”¨
model.predict_proba(X, use_tta=True, tta_rounds=5)
```
- å¤šæ¬¡dropoutæ¨ç†å¹¶å¹³å‡
- æå‡é¢„æµ‹ç¨³å®šæ€§

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹å¼1: ä½¿ç”¨è„šæœ¬ï¼ˆæ¨èï¼‰

```bash
chmod +x train_97_percent.sh
./train_97_percent.sh
```

### æ–¹å¼2: ç›´æ¥å‘½ä»¤

**å†²åˆºç‰ˆ** (6-8å°æ—¶ï¼Œ150Kæ ·æœ¬):
```bash
python main.py train \
    --model-spec bert \
    --nrows 150000 \
    --epochs 20 \
    --batch-size 16 \
    --learning-rate 1.5e-5 \
    --model-out models/model_bert_97.pt
```

**ç»ˆæç‰ˆ** (12-16å°æ—¶ï¼Œå…¨éƒ¨200K):
```bash
python main.py train \
    --model-spec bert \
    --epochs 20 \
    --batch-size 16 \
    --learning-rate 1.5e-5 \
    --model-out models/model_bert_97_ultimate.pt
```

## ğŸ”§ æŠ€æœ¯ç»†èŠ‚

### Focal LossåŸç†
```python
# æ ‡å‡†äº¤å‰ç†µ: æ‰€æœ‰æ ·æœ¬æƒé‡ç›¸åŒ
loss = -log(p_t)

# Focal Loss: éš¾æ ·æœ¬æƒé‡æ›´å¤§
loss = -(1 - p_t)^Î³ * log(p_t)

# å½“p_té«˜(æ˜“åˆ†ç±»): (1-p_t)å°ï¼Œlossè¢«é™æƒ
# å½“p_tä½(éš¾åˆ†ç±»): (1-p_t)å¤§ï¼Œlossè¢«åŠ æƒ
```

### TTA (Test-Time Augmentation)
```python
# å¯ç”¨dropoutè¿›è¡Œå¤šæ¬¡æ¨ç†
model.train()  # å¯ç”¨dropout
predictions = []
for _ in range(5):
    pred = model(x)
    predictions.append(pred)

# å¹³å‡é¢„æµ‹
final_pred = mean(predictions)
```

## ğŸ’¡ ä½¿ç”¨TTAæå‡æ¨ç†

ä¿®æ”¹ `infer.py` æˆ–ç›´æ¥è°ƒç”¨ï¼š

```python
from models.bert_model import BERTTextClassifier

# åŠ è½½æ¨¡å‹
model = BERTTextClassifier.load("models/model_bert_97.pt")

# æ ‡å‡†é¢„æµ‹
preds = model.predict(test_texts)

# ä½¿ç”¨TTAé¢„æµ‹ (æ›´å‡†ç¡®ä½†æ…¢3-5å€)
probs = model.predict_proba(test_texts, use_tta=True, tta_rounds=5)
preds = label_encoder.inverse_transform(probs.argmax(axis=1))
```

## æ¨ç†å‘½ä»¤

```bash
# æ ‡å‡†æ¨ç†
python main.py infer \
    --model models/model_bert_97.pt \
    --model-type bert \
    --input-csv data/test_a.csv \
    --output-csv predictions.csv
```

## âš™ï¸ å¦‚æœè¿˜æƒ³æ›´é«˜

### 1. é›†æˆå­¦ä¹  (+0.5-1%)
```bash
# è®­ç»ƒ3ä¸ªæ¨¡å‹
for i in 1 2 3; do
    python main.py train --model-spec bert --model-out models/bert_$i.pt
done

# æŠ•ç¥¨æˆ–å¹³å‡ï¼ˆéœ€è¦è‡ªå·±å®ç°ï¼‰
```

### 3. æ›´å¤§æ¨¡å‹ (+0.3-0.5%)
```python
# åœ¨bert_model.pyä¸­ä¿®æ”¹
d_model = 1024      # BERT-largeçº§åˆ«
num_layers = 12     # æ›´æ·±
num_heads = 16      # æ›´å¤šå¤´
```
âš ï¸ ä½†ä¼šæ…¢å¾ˆå¤šï¼Œéœ€è¦æ›´å¼ºGPU

### 4. é¢„è®­ç»ƒ+å¾®è°ƒ (+1-2%)
- ä½¿ç”¨ç›¸å…³é¢†åŸŸçš„é¢„è®­ç»ƒBERT
- ç„¶ååœ¨æœ¬ä»»åŠ¡ä¸Šå¾®è°ƒ


```bash
# å†²åˆºç‰ˆ (6-8å°æ—¶)
python main.py train --model-spec bert --nrows 150000 --epochs 20 --batch-size 16 --learning-rate 1.5e-5 --model-out models/model_bert_97.pt
```


```python
# åœ¨ä»£ç ä¸­å¯ç”¨TTA
predict_proba(texts, use_tta=True, tta_rounds=5)
```
