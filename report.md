# tianchi-nlp-news-classification report


## 训练bert模型

### 方案

#### 模型架构

*

## 从bertbase模型继续训练

### 方案

这种方法实际执行下效果非常差，准确率只有0.3。可以猜测原因如下：

#### BERT 对数字密文其实毫无先验，微调收益非常有限

* 预训练模型：`bert-base-chinese`
* 输入： **经过加密的数字 token** ，和自然语言完全没关系

从 BERT 视角看：预训练的 embedding 和 encoder 层里语义知识的几乎派不上用场；且在大量位置上还需要用随机初始化的新 token embedding；这本质上是在用一个带着一堆“无关先验”的网络去学习一堆随机编号序列上的分类问题。

所以才会导致收敛更难，初始阶段模型对这些 token 的表示是纯随机的，需要大量数据 + 训练步数才能把 embedding 和高层适配好。同时内容加密其实也把 BERT 能利用的语义信息全抹掉了。
